---
title: "Exploratory data Analysis on HIV patients data"
author: "Carlos Ramirez Alvarez"
date: "8/30/2019"
output:
  pdf_document: default
  html_document: default
---

## Loading data

```{r message=FALSE, warning=FALSE}
## Data analysis on data from flowjo
setwd('~/sc/floJoVIHDataAnalysis/sc/')
library(mice)
library(VIM)
library(dplyr)
library(missForest)
library(e1071)
library(Hmisc)

vih <- read.table('../data/PolarizationANDcommitmentCD4Tc20180911.csv',
                  sep = '\t', 
                  header = TRUE)

```

## Evaluating the number or missing data

From the follwing figure it can be seen that vast majority of variables has 10% or less missing data. Hence, 0.1 could be used as threshold for removing variables with so many missing data.

```{r message=FALSE, warning=FALSE, results='hide', fig.width=10, fig.height=7}
#md.pattern(vih[vih$Group==' HIV', ])
invisible(
aggr(vih[vih$Group==' HIV', ], 
    col=c('navyblue','yellow'),
    numbers=TRUE, 
    sortVars=TRUE,
    labels=names(vih), 
    cex.axis=.7,
     gap=3, 
     ylab=c("Missing data","Pattern"))
)
```

## Data cleaning

Next, variables with less than 10% of missing values were selected for further analysis. ID, date and IDnumber were also removed.

```{r message=FALSE, warning=FALSE, warning=FALSE}
vihCases <- vih[vih$Group==' HIV', ]

## Getting variables with < 0.3 NAs
naThresholdVar <- sapply(vihCases, function(x) sum(is.na(x))/nrow(vih) < 0.15)
vihProcessed <- vih[, naThresholdVar]
write.table(names(vihProcessed), 
            '../data/variablesAboveThreshold.txt',
            quote = FALSE, 
            sep = '\t', 
            row.names = FALSE,
            col.names = FALSE)

## dropping ID, Date and ID 
vihProcessed <- vihProcessed[, ! colnames(vihProcessed) %in% c('ID', 
                                                             'Date',
                                                             'IDnumber')]
```

## Data imputation

Here, two independent imputation strategies were implemented. A random forest approach and a very simple imputation strategy using median values in order we can compare two imputation methods.

### Random forest imputation.

```{r message=FALSE, warning=FALSE, warning=FALSE}
## Imputing data using random forest
vihImputedForest <- missForest(vihProcessed, 
                               ntree = 300)
vihImputedForest$OOBerror
```

NRMSE corresponds to a metric of imputation accuracy. Better results are expected to be closer to zero value. Hence, taking into account this metric the imputation accuracy on data is **low**.

### Imputation with median values

```{r}
vihImputedHmisc <- as.data.frame(sapply(vihProcessed, 
                                   impute, mean))
```

## Exploratory data analysis

The following figures shows hierarchical clustering over both sets of imputed data.

* **Random Forest imputed**

```{r}
vihImputedForest.m <- as.matrix(select(vihImputedForest$ximp, -Group))
vih.cor.f <- cor(vihImputedForest.m) 
heatmap(vih.cor.f, labCol = FALSE)
```

* **Median imputed**

```{r}
vihImputedMean.m <- as.matrix(select(vihImputedHmisc, -Group))
vih.cor.m <- cor(vihImputedMean.m) 
heatmap(vih.cor.m, labCol = FALSE)
```




